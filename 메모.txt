하나의 optimizer 에 두 개의 generator 를 담아서 학습을 진행하는데
여기서 backward 할 때 각각의 model 에 해당하는 파라미터의 기울기가 알아서 쌓이게 된다
step 하면 한 번에 학습 진행
optimizer 여러개 나눠서 쓰나 하나에 모아서 쓰나 큰 차이 없음

model.zero_grad 와 optimizer.zero_grad 에는 차이가 있음
self.G_AB.zero_grad()
self.G_BA.zero_grad()
이렇게 해도 되지만 한줄로
self.optimizerG.zero_grad()
이렇게 해도 무방

torch.Tensor 는 clone() 해주어야 함. Call by reference 처럼 동작하기 때문

스케쥴러
lr_lambda=lambda epoch: 1 - max(0, (epoch + start_epoch_num - decay_epoch_num) / (num_epoch - decay_epoch_num))
lr 1 에서 0 까지 감소 시작 epoch 부터 같은 비율로 차차 줄이기

RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
전에 본 에러 같은데 디바이스 같은곳에 놓으면 해결되는 에러
